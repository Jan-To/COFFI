{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f0eae5-da8c-470b-8b57-2c1c37174857",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "from bokeh.models import Range1d, ColumnDataSource, HoverTool, LinearColorMapper, ColorBar, WheelZoomTool, CDSView, IndexFilter, BasicTicker, FixedTicker, PrintfTickFormatter, FuncTickFormatter\n",
    "from bokeh.plotting import figure, show\n",
    "from bokeh.transform import linear_cmap\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "\n",
    "from umap import UMAP\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KDTree, BallTree\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import sklearn.datasets\n",
    "\n",
    "from scipy import linalg\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, MaxPooling2D, Conv2D, Flatten, Dropout\n",
    "from tensorflow.keras.initializers import VarianceScaling, Constant\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.metrics import CategoricalCrossentropy\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(tf.config.list_physical_devices())\n",
    "#tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714c89e-136d-4d27-a5c1-ebd7e29956ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgb_to_hex(c):\n",
    "    h = '#'\n",
    "    for i in c:\n",
    "        h = h + '{0:02X}'.format(int(i*255))\n",
    "    return h\n",
    "    \n",
    "class Params():\n",
    "    total_width = 1400\n",
    "    total_height = 500\n",
    "    bot_height = 1000\n",
    "    header_height = 32\n",
    "    emb_width = 500\n",
    "    fea_width = 160\n",
    "    tbl_width = 100\n",
    "    hor_width = total_width-(emb_width+fea_width+tbl_width)\n",
    "    red_res = 500\n",
    "    pdp_res = 50\n",
    "    dot_size = 6\n",
    "    \n",
    "    dataset_name = 'breast'\n",
    "    neighbors = 100\n",
    "    classifier = 'TensorFlow100'\n",
    "    \n",
    "    palette = []\n",
    "    cm = None\n",
    "    num_colors = 0\n",
    "    sat_palette = []\n",
    "    sat_cm = None\n",
    "    \n",
    "    def __init__(self):\n",
    "#         sat_color_list = ['#d95f02','#1b9e77','#7570b3','#e6ab02','#66a61e','#e7298a','#a6761d','#666666'] #1\n",
    "#        sat_color_list = ['#ff6023','#36c296','#5a7dcc','#e7298a','#66a61e','#e6ab02','#a6761d','#666666'] #1.1\n",
    "#         sat_color_list = ['#5da5da','#faa43a','#60bd68','#f17cb0','#b2912f','#b276b2','#decf3f','#f15854'] #2\n",
    "        sat_color_list = ['#1f77b4','#ff7f0e','#2ca02c','#d62728','#9467bd','#8c564b','#e377c2','#7f7f7f','#bcbd22','#17becf'] # Cat10\n",
    "#         color_list = ['#fc8d62','#66c2a5','#8da0cb','#ffd92f','#a6d854','#e78ac3','#e5c494','#b3b3b3'] #1\n",
    "        color_list = [rgb_to_hex(sns.light_palette(c, n_colors=6)[3]) for c in sat_color_list]\n",
    "        self.num_colors = len(color_list)\n",
    "        for i in range(self.num_colors):\n",
    "            self.palette += [rgb_to_hex(c) for c in sns.light_palette(color_list[i], n_colors=6)][1:]\n",
    "        self.sat_palette = sat_color_list\n",
    "        self.cm = ListedColormap(colors=self.palette)\n",
    "        self.sat_cm = ListedColormap(colors=self.sat_palette)\n",
    "            \n",
    "params = Params()\n",
    "\n",
    "class Dataset():\n",
    "    name = ''\n",
    "    data = None \n",
    "    datasource = ColumnDataSource()\n",
    "    view_filter = IndexFilter()\n",
    "    wrong_filter = IndexFilter()\n",
    "    view = CDSView()\n",
    "    labels = None\n",
    "    features = []\n",
    "    selected_features = []\n",
    "    classes = None\n",
    "    categories = None\n",
    "    trees = []\n",
    "    \n",
    "    bounds = []\n",
    "    point = []\n",
    "    \n",
    "    def load_dataset(self, name):\n",
    "        if name==\"iris\":\n",
    "            dataset = sklearn.datasets.load_iris()\n",
    "            data = dataset.data\n",
    "            target = dataset.target\n",
    "            features = dataset.feature_names\n",
    "            classes = dataset.target_names\n",
    "        if name==\"diabetes\":\n",
    "            X = pd.read_csv(\"./data/diabetes.csv\")\n",
    "            data = X.to_numpy()[:,:-1]\n",
    "            target = np.invert(np.array(X.to_numpy()[:,-1], dtype=bool))\n",
    "            features = X.columns[:-1]\n",
    "            classes = [\"diabetes\",\"no diabetes\"]\n",
    "        if name==\"shuttle\":\n",
    "            X = np.loadtxt(\"./data/shuttle/shuttle-train.txt\")\n",
    "            data = X[:,:-1]\n",
    "            target = X[:,-1].astype(int)-1\n",
    "            features = [\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\"]\n",
    "            classes = [\"Rad Flow\", \"Fpv Close\", \"Fpv Open\", \"High\", \"Bypass\", \"Bpv Close\", \"Bpv Open\"]\n",
    "        if name==\"robot24\":\n",
    "            X = pd.read_csv(\"./data/robot/sensor_readings_24.txt\")\n",
    "            data = X.to_numpy()[:,:-1]\n",
    "            target = X.to_numpy()[:,-1].astype(int)-1\n",
    "            features = [\"180°\",\"-165°\",\"-150°\",\"-135°\",\"-120°\",\"-105°\",\"-90°\",\"-75°\",\"-60°\",\"-45°\",\"-30°\",\"-15°\",\"0°\",\n",
    "                        \"15°\",\"30°\",\"45°\",\"60°\",\"75°\",\"90°\",\"105°\",\"120°\",\"135°\",\"150°\",\"165°\"]\n",
    "            classes = [\"Move-Forward\", \"Slight-Right-Turn\", \"Sharp-Right-Turn\", \"Slight-Left-Turn\"]\n",
    "        if name==\"mnist\":\n",
    "            (x_train, y_train), (x_test, y_test) = mnist.load_data(path=\"mnist.npz\")\n",
    "            data = x_train.reshape(x_train.shape[0], -1)\n",
    "            target = y_train\n",
    "            features = [str(i) for i in range(data.shape[1])]\n",
    "            classes = [str(i) for i in range(10)]\n",
    "        if name==\"breast\":\n",
    "            dataset = sklearn.datasets.load_breast_cancer()\n",
    "            data = dataset.data[:,:18]\n",
    "            target = dataset.target\n",
    "            features = dataset.feature_names[:18]\n",
    "            classes = dataset.target_names\n",
    "            \n",
    "        if not isinstance(features, list):\n",
    "            features = features.tolist()\n",
    "            \n",
    "        if name==\"shuttle\" or name==\"robot24\" or name==\"robot4\":\n",
    "            n = 400\n",
    "            new_data = np.zeros((0,data.shape[1]))\n",
    "            new_target = np.zeros(0, dtype=int)\n",
    "            np.random.seed(0)\n",
    "            for i in np.unique(target):\n",
    "                possible_inds = np.where(target == i)[0]\n",
    "                rnd_ind = np.random.choice(possible_inds, min(n,possible_inds.shape[0]), replace=False)\n",
    "                new_data = np.append(new_data, data[rnd_ind,:], axis=0)\n",
    "                new_target = np.append(new_target, target[rnd_ind])\n",
    "            data = new_data\n",
    "            target = new_target\n",
    "            \n",
    "        return pd.DataFrame(data, columns=features), target, features, classes\n",
    "    \n",
    "    def update(self, name):\n",
    "        self.name = name\n",
    "        self.data,self.labels,self.features,self.classes = self.load_dataset(self.name)\n",
    "        self.selected_features = self.features\n",
    "        \n",
    "        self.data['color']      = ['#444444']*len(self.data.index)\n",
    "        self.data['sat_color']  = ['#444444']*len(self.data.index)\n",
    "        self.data['line_color'] = ['#444444']*len(self.data.index)\n",
    "        self.data['size'] = [params.dot_size]*len(self.data.index)\n",
    "        self.data['x']  = [0]*len(self.data.index)\n",
    "        self.data['y']  = [0]*len(self.data.index)\n",
    "        self.data['x1'] = [0]*len(self.data.index)\n",
    "        self.data['y1'] = [0]*len(self.data.index)\n",
    "        self.datasource.selected.indices = [0] \n",
    "        self.datasource.selected.indices = []\n",
    "        self.datasource.data = self.data\n",
    "        self.view_filter.indices  = list(self.data.index)\n",
    "        self.wrong_filter.indices = list(self.data.index)\n",
    "        self.view = CDSView(source=self.datasource, filters=[self.view_filter])\n",
    "        \n",
    "#         self.bounds = [dataset.data[dataset.features].quantile(0.05).to_numpy(), dataset.data[dataset.features].quantile(0.95).to_numpy()]\n",
    "        self.bounds = [dataset.data[dataset.features].min().to_numpy(), dataset.data[dataset.features].max().to_numpy()]\n",
    "        self.point = dataset.data[dataset.features].mean()\n",
    "        print(\"--- data loaded\", self.data.shape)\n",
    "\n",
    "dataset = Dataset()\n",
    "dataset.update(params.dataset_name)\n",
    "\n",
    "class TFModel():\n",
    "    typ = None\n",
    "    \n",
    "    def __init__(self, typ, input_shape):\n",
    "        # define keras model\n",
    "        self.model = Sequential()\n",
    "        if typ == '2048':\n",
    "            self.model.add(Dense(2048, input_dim=input_shape, activation='relu'))\n",
    "            self.model.add(Dense(2048, activation='relu'))\n",
    "            self.model.add(Dense(2048, activation='relu'))\n",
    "            self.model.add(Dense(len(dataset.classes), activation='softmax'))\n",
    "            self.model.compile(loss='mse', optimizer='sgd', metrics=[\"accuracy\"])\n",
    "        elif typ == 'Conv':\n",
    "            self.typ = 'Conv'\n",
    "            self.model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "            self.model.add(MaxPooling2D((2, 2)))\n",
    "            self.model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "            self.model.add(MaxPooling2D((2, 2)))\n",
    "            self.model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "            self.model.add(Flatten())\n",
    "            self.model.add(Dropout(0.5))\n",
    "            self.model.add(Dense(len(dataset.classes), activation=\"softmax\"))\n",
    "            self.model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "        #self.model.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
    "        elif typ == '100':\n",
    "            self.model.add(Dense(100, input_dim=input_shape, activation='relu'))\n",
    "            self.model.add(Dense(100, activation='relu'))\n",
    "            self.model.add(Dense(100, activation='relu'))\n",
    "            self.model.add(Dense(len(dataset.classes), activation='softmax'))\n",
    "            self.model.compile(loss='mse', optimizer='adam', metrics=[\"accuracy\"])\n",
    "            \n",
    "    def fit(self, X, Y):\n",
    "        # encode class values as integers\n",
    "        encoder = LabelEncoder()\n",
    "        encoder.fit(Y)\n",
    "        encoded_Y = encoder.transform(Y)\n",
    "        # convert integers to dummy variables (i.e. one hot encoded)\n",
    "        dummy_y = to_categorical(encoded_Y)\n",
    "        if self.typ == 'Conv':\n",
    "            # transform to image proportions\n",
    "            dummy_x = X.reshape(-1,28,28,1)\n",
    "        else:\n",
    "            dummy_x = X\n",
    "        # train model\n",
    "        tic = time.perf_counter()\n",
    "        self.model.fit(dummy_x, dummy_y, epochs=200, batch_size=200, verbose=0)\n",
    "        toc = time.perf_counter()\n",
    "        print(\"Train Model:\",toc-tic,\"s\")\n",
    "        print('Model accuracy:', self.model.evaluate(dummy_x, dummy_y))\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        if self.typ == 'Conv':\n",
    "            # transform to image proportions\n",
    "            dummy_x = X.reshape(-1,28,28,1)\n",
    "        else:\n",
    "            dummy_x = X\n",
    "        return self.model.predict(dummy_x)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return np.argmax(self.model.predict(X), axis=1)\n",
    "\n",
    "class Predictor():\n",
    "    pip = None\n",
    "    \n",
    "    def update_pip(self, classifier_type, input_shape=None):\n",
    "        if classifier_type == \"RandomForest\":\n",
    "            self.pip = Pipeline([('scaler', StandardScaler()), \n",
    "                                 ('classificator', RandomForestClassifier(\n",
    "                                      random_state=1, min_samples_split = 4, min_samples_leaf = 3))])\n",
    "        elif classifier_type == \"NeuralNetwork\":\n",
    "            self.pip = Pipeline([('scaler', StandardScaler()), \n",
    "                                 ('classificator', MLPClassifier(hidden_layer_sizes=(100,100,100), \n",
    "                                                                 max_iter=5000, random_state=1))])\n",
    "        elif classifier_type == \"NeuralNetworkBig\":\n",
    "            self.pip = Pipeline([('scaler', StandardScaler()), \n",
    "                                 ('classificator', MLPClassifier(hidden_layer_sizes=(2048,2048,2048), \n",
    "                                                                 max_iter=5000, random_state=1))])\n",
    "        elif classifier_type == \"TensorFlow100\":\n",
    "            self.pip = Pipeline([('scaler', StandardScaler()), ('classificator', TFModel('100',input_shape))])\n",
    "        elif classifier_type == \"TensorFlowConv\":\n",
    "            self.pip = Pipeline([('scaler', StandardScaler()), ('classificator', TFModel('Conv',input_shape))])\n",
    "        elif classifier_type == \"TensorFlow2048\":\n",
    "            self.pip = Pipeline([('scaler', StandardScaler()), ('classificator', TFModel('2048',input_shape))])\n",
    "        else: print(\"unkown classifier type:\", classifier_type)\n",
    "        \n",
    "    def update_data(self, dataset, params):\n",
    "        # train predictor on dataset\n",
    "        tic = time.perf_counter()\n",
    "        self.pip.fit(dataset.data[dataset.features], dataset.labels)\n",
    "        toc = time.perf_counter()\n",
    "        print(\"--- predictor trained:\", toc-tic,\"s\")\n",
    "        # save predicitons in dataset\n",
    "        dataset.data['prob'] = self.pip.predict_proba(dataset.data[dataset.features]).tolist()\n",
    "        dataset.data['maxprob'] = np.max(np.array(dataset.data['prob'].tolist()), axis=1)\n",
    "        dataset.datasource.data['prob'] = dataset.data['prob']\n",
    "        dataset.datasource.data['maxprob'] = dataset.data['maxprob']\n",
    "        # compute colors of dataset\n",
    "        dataset.data['most_prob_class'] = np.argmax(np.array(dataset.data['prob'].tolist()), axis=1)\n",
    "        colorvalues = (np.clip(dataset.data['maxprob'].to_numpy(),0.51,0.99) - 0.5) * 2 + dataset.data['most_prob_class'].to_numpy()\n",
    "        dataset.data['color'] = [rgb_to_hex(params.cm(i/params.num_colors)) for i in colorvalues]\n",
    "        dataset.data['sat_color'] = [params.sat_palette[i] for i in dataset.data['most_prob_class'].to_numpy()]\n",
    "        dataset.data['line_color'] = dataset.data['sat_color']\n",
    "        dataset.datasource.data['color'] = dataset.data['color']\n",
    "        dataset.datasource.data['sat_color'] = dataset.data['sat_color']\n",
    "        dataset.datasource.data['line_color'] = dataset.data['sat_color']\n",
    "        \n",
    "        # compute wrongly predicted indices\n",
    "        dataset.wrong_filter.indices = np.where(np.not_equal(dataset.data['most_prob_class'].to_numpy(), dataset.labels))[0].tolist()\n",
    "#         dataset.datasource.data['target_color'] = np.where(\n",
    "#             dataset.data['most_prob_class'].to_numpy() != dataset.labels,\n",
    "#             [rgb_to_hex(params.sat_cm(i/params.num_colors)) for i in (dataset.labels+0.99)], \n",
    "#             np.full(len(dataset.data),'#333333'))\n",
    "        dataset.datasource.data['target_color'] = [params.sat_palette[i] for i in dataset.labels]\n",
    "    \n",
    "        # create nearest neighbor search structure per class\n",
    "        dataset.trees = []\n",
    "        for i in range(len(dataset.classes)):\n",
    "            index_map = np.where(dataset.data['most_prob_class'].to_numpy() == i)[0]\n",
    "            if len(index_map) == 0: \n",
    "                dataset.trees.append(([], None))\n",
    "                continue\n",
    "            class_data = dataset.data.loc[index_map,dataset.features]\n",
    "            scaled_class_data = self.pip['scaler'].transform(class_data)\n",
    "            dataset.trees.append( (index_map, BallTree(scaled_class_data)) )\n",
    "        \n",
    "#import tensorflow as tf\n",
    "#if tf.test.gpu_device_name(): \n",
    "#    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "#else:\n",
    "#    print(\"Please install GPU version of TF\")\n",
    "\n",
    "predictor = Predictor()\n",
    "predictor.update_pip(params.classifier, input_shape=len(dataset.features))\n",
    "predictor.update_data(dataset, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1857017b-6efe-441a-b1cc-e05a9aeb6e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from embedding_util2_1 import Shifter, SVD\n",
    "\n",
    "class Embedding():\n",
    "    \n",
    "    def __init__(self, dataset, nn_inds=[], point=None):\n",
    "        self.emb = Pipeline([('shifter', Shifter()), ('scaler', StandardScaler()), ('pca', SVD())])\n",
    "        \n",
    "        if len(nn_inds) < 1:\n",
    "            nn_inds = dataset.data.index.tolist()\n",
    "               \n",
    "        train_set = dataset.data.loc[nn_inds,dataset.selected_features]\n",
    "        self.emb.fit(train_set)\n",
    "            \n",
    "        shift_bounds = [[dataset.bounds[j][i] for i,f in enumerate(dataset.features) if (f in dataset.selected_features)] for j in [0,1]]\n",
    "        self.emb['shifter'].set_bounds(shift_bounds) \n",
    "        if point is not None:\n",
    "            # shift plane to focus point\n",
    "            shift = point[dataset.selected_features].values.astype('float64') - self.emb.inverse_transform(self.emb.transform([point[dataset.selected_features]]))[0]\n",
    "            self.emb['shifter'].set_by(shift)\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.emb.fit(X, y)\n",
    "            \n",
    "    def transform(self, X):\n",
    "        return self.emb.transform(X)\n",
    "    \n",
    "    def inverse_transform(self, X):\n",
    "        return self.emb.inverse_transform()\n",
    "    \n",
    "    \n",
    "def update_non_linear_view(p, predictor, method=\"PCA\", neighbors=20, sel_feats_only=False, reverse='PCA'):\n",
    "    dataset.selected_features = dataset.features\n",
    "    # reduce data with non-linear DR\n",
    "    if method == \"UMAP\":\n",
    "        nl_pip = Pipeline([('scaler', StandardScaler()), ('reducer', UMAP(n_neighbors=neighbors, random_state=42))])\n",
    "    elif method == \"t-SNE\":\n",
    "        nl_pip = Pipeline([('scaler', StandardScaler()), ('reducer', TSNE(perplexity=neighbors, random_state=42))])\n",
    "    elif method == \"PCA\":\n",
    "        embedding = Embedding(dataset)\n",
    "        nl_pip = Pipeline([('scaler', embedding.emb[:2]), ('reducer', embedding.emb['pca'])])\n",
    "    if sel_feats_only:\n",
    "        reduced = nl_pip.fit_transform(dataset.data[dataset.selected_features])\n",
    "    else:\n",
    "        tic = time.perf_counter()\n",
    "        reduced = nl_pip.fit_transform(dataset.data[dataset.features])\n",
    "        toc = time.perf_counter()\n",
    "        print(method,dataset.data.shape[0]//1000,\"K samples:\",toc-tic,\"s\")\n",
    "    \n",
    "    dataset.data['x1'] = reduced[:,0]\n",
    "    dataset.data['y1'] = reduced[:,1]\n",
    "    \n",
    "    # update plot bounds\n",
    "    bx = max(dataset.data['x1'])-min(dataset.data['x1'])\n",
    "    by = max(dataset.data['y1'])-min(dataset.data['y1'])\n",
    "    x_min = min(dataset.data['x1'])-0.1*bx\n",
    "    x_max = max(dataset.data['x1'])+0.1*bx\n",
    "    y_min = min(dataset.data['y1'])-0.1*by\n",
    "    y_max = max(dataset.data['y1'])+0.1*by\n",
    "    p.x_range.update(start=x_min, end=x_max)\n",
    "    p.y_range.update(start=y_min, end=y_max)\n",
    "    \n",
    "    # update plot datasource\n",
    "    #dataset.datasource.data['x1'] = reduced[:,0]\n",
    "    #dataset.datasource.data['y1'] = reduced[:,1]\n",
    "    \n",
    "    ### create background map ###\n",
    "    \n",
    "    # create samples\n",
    "    N = 200\n",
    "    px = np.linspace(x_min, x_max, num=N)\n",
    "    py = np.linspace(y_min, y_max, num=N)\n",
    "    xx, yy = np.meshgrid(px,py)\n",
    "    grid = np.c_[xx.ravel(), yy.ravel()]\n",
    "    \n",
    "    ### iLAMP ###\n",
    "    if reverse == 'iLAMP':\n",
    "        # find map-neighbors\n",
    "        kdtree = KDTree(reduced)\n",
    "\n",
    "        tic = time.perf_counter()\n",
    "        invp = np.zeros((N*N, len(dataset.features)))\n",
    "        for i, point in enumerate(grid):\n",
    "            dists, Ys_inds = kdtree.query(point.reshape(1,-1), k=20)\n",
    "            # remove queried point from set\n",
    "            dists, Ys_inds = dists.T, Ys_inds[0]\n",
    "            Ys = reduced[Ys_inds]\n",
    "            Xs = nl_pip['scaler'].transform(dataset.data.loc[Ys_inds,dataset.features].to_numpy())\n",
    "            # equation (2) + eliminate division by zero\n",
    "            alphas = 1 / np.maximum(np.square(dists), 0.001)\n",
    "            alpha_sum = np.sum(alphas)\n",
    "            # equation (3)\n",
    "            x_tilde = np.sum(alphas * Xs, axis=0) / alpha_sum\n",
    "            y_tilde = np.sum(alphas * Ys, axis=0) / alpha_sum\n",
    "            # equation (4)\n",
    "            x_hat = Xs - x_tilde\n",
    "            y_hat = Ys - y_tilde\n",
    "            # equation (5)\n",
    "            A = np.sqrt(alphas) * y_hat\n",
    "            B = np.sqrt(alphas) * x_hat\n",
    "            # equation (7)\n",
    "            U, D, VH = linalg.svd(A.T @ B, full_matrices=False)\n",
    "            M = U @ VH\n",
    "            # equation (8)\n",
    "            f_p = ((point - y_tilde) @ M) + x_tilde\n",
    "            invp[i] = f_p\n",
    "            if (i-1) % 1000 == 0: \n",
    "                tuc = time.perf_counter()\n",
    "                print(\"iLAMP:\",i,\"/\",N*N,\"in\",tuc-tic,\"s\")\n",
    "        invp = nl_pip['scaler'].inverse_transform(invp)\n",
    "        toc = time.perf_counter()\n",
    "        print(\"iLAMP Samples:\",toc-tic,\"s\")\n",
    "        \n",
    "        \n",
    "    elif reverse == 'NNInv':\n",
    "        ### NNInv ###\n",
    "\n",
    "        # prepare data transformations\n",
    "        unit_scaler = MinMaxScaler()\n",
    "        if sel_feats_only:\n",
    "            target = unit_scaler.fit_transform(dataset.data[dataset.selected_features])\n",
    "        else:\n",
    "            target = unit_scaler.fit_transform(dataset.data[dataset.features])\n",
    "\n",
    "        # define keras model\n",
    "    #     var_init = VarianceScaling(scale=1.0, mode=\"fan_in\", distribution=\"uniform\", seed=42)\n",
    "    #     const_init = Constant(0.01)\n",
    "        model = Sequential()\n",
    "        if False:\n",
    "            model.add(Dense(2048, input_dim=2, activation='relu'))\n",
    "            model.add(Dense(2048, activation='relu'))\n",
    "            model.add(Dense(2048, activation='relu'))\n",
    "        else:\n",
    "            model.add(Dense(100, input_dim=2, activation='relu'))\n",
    "            model.add(Dense(100, activation='relu'))\n",
    "            model.add(Dense(100, activation='relu'))\n",
    "        if sel_feats_only:\n",
    "            model.add(Dense(len(dataset.selected_features), activation='sigmoid'))\n",
    "        else:\n",
    "            model.add(Dense(len(dataset.features), activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "        # train model\n",
    "        permutation = np.random.rand(reduced.shape[0]).argsort()\n",
    "        earlystop = EarlyStopping(patience=5)\n",
    "\n",
    "        tic = time.perf_counter()\n",
    "        #model.fit(reduced[permutation], target[permutation], epochs=150, batch_size=32, validation_split=0.25, callbacks=[earlystop])\n",
    "        model.fit(reduced[permutation], target[permutation], epochs=150, batch_size=200, verbose=0)\n",
    "        \n",
    "        toc = time.perf_counter()\n",
    "        print(\"Train Model:\",toc-tic,\"s\")\n",
    "\n",
    "        # save model\n",
    "        #model.save('NNInv')\n",
    "\n",
    "        # load model\n",
    "        #model = load_model('NNInv')\n",
    "\n",
    "        # infer samples\n",
    "        tic = time.perf_counter()\n",
    "        invp = model.predict(grid)\n",
    "        invp = unit_scaler.inverse_transform(invp)\n",
    "        toc = time.perf_counter()\n",
    "        print(\"Infer\",(N*N)/1000,\"K samples:\",toc-tic,\"s\")\n",
    "    \n",
    "    elif reverse == 'PCA' or reverse == 'PCA_filtered':\n",
    "        invp = nl_pip.inverse_transform(grid)\n",
    "    \n",
    "    # predict samples\n",
    "    tic = time.perf_counter()\n",
    "    probs = predictor.predict_proba(invp)\n",
    "    toc = time.perf_counter()\n",
    "    print(\"Predict Samples:\",toc-tic,\"s\")\n",
    "    \n",
    "    # update image\n",
    "    img_src = (np.clip(np.max(probs, axis=1),0.501,0.999) - 0.5) * 2 + np.argmax(probs, axis=1) \n",
    "    source = p.select('image').data_source\n",
    "    source.data['image'] = [img_src.reshape((N,N))]\n",
    "    source.data['x'] = [x_min]\n",
    "    source.data['y'] = [y_min]\n",
    "    source.data['dw'] = [x_max-x_min]\n",
    "    source.data['dh'] = [y_max-y_min]\n",
    "    \n",
    "    def bfs(img,start):\n",
    "        # Breadth-first search to find pixels that are True\n",
    "        if img[start[1],start[0]]:\n",
    "            return start\n",
    "        visited = set()\n",
    "        w, h = img.shape[0]-1, img.shape[1]-1\n",
    "        queue = [start]\n",
    "        while queue:\n",
    "            x, y = queue.pop(0)\n",
    "            neighbors = []\n",
    "            if x<w:\n",
    "                neighbors.append((x+1,y))\n",
    "            if x>0:\n",
    "                neighbors.append((x-1,y))\n",
    "            if y<h:\n",
    "                neighbors.append((x,y+1))\n",
    "            if y>0:\n",
    "                neighbors.append((x,y-1))   \n",
    "            for n in neighbors:\n",
    "                if img[n[1],n[0]]:\n",
    "                    return n\n",
    "                if n not in visited:\n",
    "                    visited.add(n)\n",
    "                    queue.append(n)\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    cf_pix_diff = np.zeros(len(dataset.data))\n",
    "    cf_diff = np.zeros(len(dataset.data))\n",
    "    cfs = np.zeros((len(dataset.data),len(dataset.features)))\n",
    "    \n",
    "    if reverse == 'PCA' or reverse == 'PCA_filtered':\n",
    "        for i, point in dataset.data.iterrows():\n",
    "        #for i, point in [(0,dataset.data.loc[90,:])]:\n",
    "            # find balanced NN\n",
    "            def find_nn(p, num_nn=100):\n",
    "                # find nearest neighbors distributed: 1/2 own class, 1/2 other classes\n",
    "                own_class = np.argmax(p['prob']) if p.name != None else np.argmax(predictor.pip.predict_proba([p[dataset.features]])[0])\n",
    "                own_nn = np.array([])\n",
    "                other_dist = np.full((len(dataset.classes), num_nn//2), np.inf)\n",
    "                other_nn   = np.full((len(dataset.classes), num_nn//2), -1)\n",
    "                for j in range(len(dataset.classes)):\n",
    "                    if dataset.trees[j][1] is None:\n",
    "                        continue\n",
    "                    dist, ind = dataset.trees[j][1].query(\n",
    "                        predictor['scaler'].transform([p[dataset.features]]), k=min(num_nn//2,len(dataset.trees[j][0])))\n",
    "                    if j == own_class:\n",
    "                        own_nn = np.append(own_nn, dataset.trees[j][0][ind])\n",
    "                    else:\n",
    "                        other_dist[j,:dist.shape[1]] = dist[0]\n",
    "                        other_nn[j,:dist.shape[1]] = dataset.trees[j][0][ind[0]]\n",
    "                other_nn = other_nn[np.unravel_index(np.argsort(other_dist, axis=None), other_dist.shape)]\n",
    "                other_nn = np.delete(other_nn, np.where(other_nn == -1))\n",
    "                other_nn = other_nn[:min(num_nn//2, len(other_nn))]\n",
    "                nn = np.append(own_nn, other_nn).astype(int).tolist()\n",
    "                return nn\n",
    "            t1 = time.perf_counter()\n",
    "            nn_inds = find_nn(point)\n",
    "            t2 = time.perf_counter()\n",
    "            \n",
    "            # find important features\n",
    "            if reverse == 'PCA_filtered':\n",
    "                importances = permutation_importance(predictor, dataset.data.loc[nn_inds,dataset.features].to_numpy(), \n",
    "                                                     np.argmax(np.array(dataset.data.loc[nn_inds,'prob'].tolist()), axis=1),\n",
    "                                                     n_repeats=5, random_state=0, scoring='accuracy').importances_mean\n",
    "                if np.any((importances != 0)):\n",
    "                    importances /= np.sum(importances)\n",
    "                else:\n",
    "                    print('ERROR: computing importances lead to zeros only')\n",
    "                important_indices = np.where(importances > 0.8/len(dataset.features))[0]\n",
    "                #for j in range(10):\n",
    "                #    if len(important_indices) >= 2: break\n",
    "                #    print('strong importance distribution', i, importances)\n",
    "                #    important_indices = np.where(importances > (0.5**j)/len(dataset.features))[0]\n",
    "                #    print('take indices', important_indices)\n",
    "                if len(important_indices) < 2:\n",
    "                    print(i, 'take indices', important_indices)\n",
    "                dataset.selected_features = np.array(dataset.features)[important_indices].tolist()\n",
    "            else:\n",
    "                important_indices = list(range(len(dataset.features)))\n",
    "                \n",
    "            # create neighborhood embedding\n",
    "            t3 = time.perf_counter()\n",
    "            nn_emb = Embedding(dataset, nn_inds, point)\n",
    "            t4 = time.perf_counter()\n",
    "            nn_pip = Pipeline([('scaler', nn_emb.emb[:2]), ('reducer', nn_emb.emb['pca'])])\n",
    "            \n",
    "            # adjust image to local version\n",
    "            reduced = np.zeros((len(dataset.data),2))\n",
    "            reduced[nn_inds,:] = nn_pip.transform(dataset.data[dataset.selected_features])[nn_inds,:]\n",
    "            x_min = min(reduced[:,0])\n",
    "            x_max = max(reduced[:,0])\n",
    "            y_min = min(reduced[:,1])\n",
    "            y_max = max(reduced[:,1])\n",
    "            \n",
    "            # update plot bounds\n",
    "            bx = x_max-x_min\n",
    "            by = y_max-y_min\n",
    "            x_min -= (max(bx,by)/bx -1) * bx/2\n",
    "            x_max += (max(bx,by)/bx -1) * bx/2\n",
    "            y_min -= (max(bx,by)/by -1) * by/2\n",
    "            y_max += (max(bx,by)/by -1) * by/2\n",
    "            bounds_range = max(x_max-x_min, y_max-y_min)\n",
    "            \n",
    "            t5 = time.perf_counter()\n",
    "            # create samples\n",
    "            def compute_and_predict_grid(dataset, inv_tf_fn, predict_fn, N, x_min, x_max, y_min, y_max):\n",
    "                px = np.linspace(x_min, x_max, num=N)\n",
    "                py = np.linspace(y_min, y_max, num=N)\n",
    "                xx, yy = np.meshgrid(px,py)\n",
    "\n",
    "                tic = time.perf_counter()\n",
    "                nn_invp = inv_tf_fn(np.c_[xx.ravel(), yy.ravel()])\n",
    "                tac = time.perf_counter()\n",
    "\n",
    "                for i,f in enumerate(dataset.features):\n",
    "                    if f not in dataset.selected_features:\n",
    "                        nn_invp = np.insert(nn_invp, i, [point[i]]*len(nn_invp), axis=1)\n",
    "\n",
    "                probs = predict_fn(nn_invp)\n",
    "                toc = time.perf_counter()\n",
    "                #print(\"inv_points  \",tac-tic,\"s\")\n",
    "                #print(\"predict     \",toc-tac,\"s\")\n",
    "                return probs, nn_invp\n",
    "\n",
    "            for j in range(1,6):    \n",
    "                nn_probs, nn_invp = compute_and_predict_grid(dataset, nn_pip.inverse_transform, predictor.predict_proba, N, x_min, x_max, y_min, y_max)\n",
    "                visible_class_count = np.count_nonzero(np.amax(nn_probs, axis=0) > 0.5)\n",
    "                if visible_class_count >= 2: \n",
    "                    break\n",
    "                print(i, 'needs more iterations:', j)\n",
    "                x_min -= (j/2.5)*bounds_range\n",
    "                x_max += (j/2.5)*bounds_range\n",
    "                y_min -= (j/2.5)*bounds_range\n",
    "                y_max += (j/2.5)*bounds_range\n",
    "            \n",
    "            img = ((np.clip(np.max(nn_probs, axis=1),0.501,0.999) - 0.5) * 2 + np.argmax(nn_probs, axis=1)).reshape((N,N))\n",
    "            t6 = time.perf_counter()\n",
    "            # adjust image to local version\n",
    "            #dataset.datasource.data['x1'] = reduced[:,0]\n",
    "            #dataset.datasource.data['y1'] = reduced[:,1]\n",
    "            #sizes = len(dataset.data)*[15]\n",
    "            #sizes[i] = params.dot_size\n",
    "            #dataset.datasource.data['size'] = sizes#[params.dot_size if i!=j else 15 for j in range(len(dataset.data))]\n",
    "            t7 = time.perf_counter()    \n",
    "            # find next cf-pixel\n",
    "            pix_pos = ((((nn_pip.transform([point[dataset.selected_features]]) - [x_min,y_min]) / [x_max-x_min, y_max-y_min]) * N) - 0.001).astype(int)[0]\n",
    "            p_class = point['most_prob_class']\n",
    "            img_cp = np.logical_not(np.logical_and(p_class < img, img < (p_class + 1)))\n",
    "            #print('i', i, 'class', p_class, 'p', point, 'img', img[point[1], point[0]], 'img_cp', img_cp[point[1], point[0]])\n",
    "            cf_pix = bfs(img_cp, pix_pos)\n",
    "            t8 = time.perf_counter()    \n",
    "            if cf_pix is None:\n",
    "                print('could not find cf in image of', i)\n",
    "                source = p.select('image').data_source\n",
    "                source.data['image'] = [img.reshape((N,N))]\n",
    "                source.data['x'] = [x_min]\n",
    "                source.data['y'] = [y_min]\n",
    "                source.data['dw'] = [x_max-x_min]\n",
    "                source.data['dh'] = [y_max-y_min]\n",
    "                p.x_range.update(start=x_min, end=x_max)\n",
    "                p.y_range.update(start=y_min, end=y_max)\n",
    "                cf_pix = pix_pos\n",
    "            cf_pix_diff[i] = np.linalg.norm(pix_pos - cf_pix) * (1 + (j-1)/2.5)\n",
    "            cfs[i,:] = nn_invp.reshape(N,N,len(dataset.features))[cf_pix[0],cf_pix[1]]\n",
    "            cf_diff[i] = np.linalg.norm(nn_pip['scaler'].transform(dataset.data.loc[i,dataset.selected_features].to_numpy().reshape(1,-1))[0] \n",
    "                                      - nn_pip['scaler'].transform(nn_invp.reshape(N,N,len(dataset.features))[cf_pix[0],cf_pix[1],important_indices].reshape(1,-1))[0], ord=1)\n",
    "            if i % 100 == 0:\n",
    "                print('individual PCA progress:', i,'/', len(dataset.data))\n",
    "            \n",
    "            t9 = time.perf_counter()    \n",
    "            source = p.select('image').data_source\n",
    "            source.data['image'] = [img.reshape((N,N))]\n",
    "            source.data['x'] = [x_min]\n",
    "            source.data['y'] = [y_min]\n",
    "            source.data['dw'] = [x_max-x_min]\n",
    "            source.data['dh'] = [y_max-y_min]\n",
    "            p.x_range.update(start=x_min, end=x_max)\n",
    "            p.y_range.update(start=y_min, end=y_max)\n",
    "            if False:\n",
    "                print('Find NN      ', t2-t1)\n",
    "                print('Compute Impo.', t3-t2)\n",
    "                print('Create Emb.  ', t4-t3)\n",
    "                print('reduce points', t5-t4)\n",
    "                print('invp+predict ', t6-t5)\n",
    "                print('pix_pos & img', t7-t6)\n",
    "                print('find img cf  ', t8-t7)\n",
    "                print('write diffs  ', t9-t8)\n",
    "    else: \n",
    "        # find next cf-pixel\n",
    "        img = img_src.reshape((N,N))\n",
    "        pix_pos = ((((reduced - [x_min,y_min]) / [x_max-x_min, y_max-y_min]) * N) - 0.001).astype(int)\n",
    "        for i, point in enumerate(pix_pos):\n",
    "            p_class = dataset.data['most_prob_class'][i]\n",
    "            img_cp = np.logical_not(np.logical_and(p_class < img, img < (p_class + 1)))\n",
    "            #print('i', i, 'class', p_class, 'p', point, 'img', img[point[1], point[0]], 'img_cp', img_cp[point[1], point[0]])\n",
    "            cf_pix = bfs(img_cp, point)\n",
    "            cf_pix_diff[i] = np.linalg.norm(point - cf_pix)\n",
    "            cfs[i,:] = invp.reshape(N,N,len(dataset.features))[cf_pix[0],cf_pix[1]]\n",
    "            cf_diff[i] = np.linalg.norm(predictor['scaler'].transform(dataset.data.loc[i,dataset.features].to_numpy().reshape(1,-1))[0]  - \n",
    "                                        predictor['scaler'].transform(invp.reshape(N,N,len(dataset.features))[cf_pix[0],cf_pix[1]].reshape(1,-1))[0], ord=1)\n",
    "        \n",
    "    np.save('benchmarks/'+params.dataset_name+'/IMG_cfs_pix_diff_L2_'+params.dataset_name+'_'+params.classifier+'_'+reverse,cf_pix_diff)\n",
    "    np.save('benchmarks/'+params.dataset_name+'/IMG_cfs_diff_L1_'+params.dataset_name+'_'+params.classifier+'_'+reverse,cf_diff)\n",
    "    np.save('benchmarks/'+params.dataset_name+'/IMG_cfs_'+params.dataset_name+'_'+params.classifier+'_'+reverse,cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e504ff87-ae5e-431d-a1ef-18e1f8acbc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find mathematical cf\n",
    "from alibi.explainers import CounterFactual\n",
    "\n",
    "shape = (1,) + (len(dataset.features),)\n",
    "model = predictor.pip['classificator'].model\n",
    "mins = predictor.pip['scaler'].transform([dataset.data[dataset.features].min(0)])[0]\n",
    "maxs = predictor.pip['scaler'].transform([dataset.data[dataset.features].max(0)])[0]\n",
    "ranges = [(i,j) for i,j in zip(mins,maxs)] # (-1e10, 1e10)\n",
    "\n",
    "# You cannot re-run this cell due to alibi. Their abstract constructor can only be called once.\n",
    "cf_generator = CounterFactual(model, shape, distance_fn='l1', target_proba=0.56,\n",
    "                    target_class='other', max_iter=500, early_stop=50, lam_init=0.1,\n",
    "                    max_lam_steps=20, tol=0.05, learning_rate_init=0.1,\n",
    "                    feature_range=(-1e10, 1e10), eps=0.05, init='identity',\n",
    "                    decay=True, write_dir=None, debug=False)\n",
    "cfs = np.zeros((len(dataset.data),len(dataset.features)))\n",
    "dists = np.zeros(len(dataset.data))\n",
    "tic = time.perf_counter()\n",
    "for index, point in enumerate(predictor.pip['scaler'].transform(dataset.data.loc[:,dataset.features])):\n",
    "    expl = cf_generator.explain(point.reshape(1,-1))\n",
    "    if expl.cf is None:\n",
    "        print(index, 'could not find optimal cf,  img-pos:', predictor.pip['scaler'].inverse_transform(point), ' point', point)\n",
    "        continue\n",
    "    else:\n",
    "        cfs[index] = expl.cf['X']\n",
    "        dists[index] = expl.cf['distance']\n",
    "        \n",
    "    if index % 50 == 0 or index < 10: \n",
    "        toc = time.perf_counter()\n",
    "        print('Found cf for index', index, '/', len(dataset.data),' total time:', int(toc-tic), 's   with L1-distance:', expl.cf['distance'])\n",
    "        np.save('benchmarks/'+params.dataset_name+'/cfs_L1_'+params.dataset_name+'_'+params.classifier+'.npy', cfs)\n",
    "        np.save('benchmarks/'+params.dataset_name+'/dists_L1_'+params.dataset_name+'_'+params.classifier+'.npy', dists)\n",
    "    #predictor.pip['scaler'].inverse_transform(expl.cf['X'])\n",
    "np.save('benchmarks/'+params.dataset_name+'/cfs_L1_'+params.dataset_name+'_'+params.classifier+'.npy', cfs)\n",
    "np.save('benchmarks/'+params.dataset_name+'/dists_L1_'+params.dataset_name+'_'+params.classifier+'.npy', dists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999f89d6-ae41-47ae-83cf-b33cfef84b81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5082dd8-d963-4e23-abff-955ce6e0e171",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = figure(height=params.bot_height, width=params.bot_height+30, tools=\"pan,tap,box_select,lasso_select\", x_range=(-1,1), y_range=(-1,1))\n",
    "    \n",
    "p.add_tools(HoverTool(names=[\"scatter\"], name='hover', tooltips = [(\"id\", \"$index\")]), WheelZoomTool(zoom_on_axis=False))\n",
    "\n",
    "p.image(image=[[]], color_mapper=LinearColorMapper(palette=params.palette, low=0, high=params.num_colors), \n",
    "        name='image', x=[0], y=[0], dw=[0], dh=[0], level='underlay')\n",
    "\n",
    "update_non_linear_view(p, predictor.pip, method='UMAP', reverse='iLAMP')\n",
    "update_non_linear_view(p, predictor.pip, method='UMAP', reverse='NNInv')\n",
    "update_non_linear_view(p, predictor.pip, method='PCA', reverse='PCA')\n",
    "update_non_linear_view(p, predictor.pip, method='PCA', reverse='PCA_filtered')\n",
    "\n",
    "mapper = linear_cmap(field_name='sat_color', palette=params.palette, low=0, high=params.num_colors)\n",
    "p.scatter(source=dataset.datasource, x='x1', y='y1', color='sat_color', line_color='line_color', \n",
    "          size='size', name=\"scatter\", nonselection_fill_alpha=0.1, nonselection_line_alpha=0.1)\n",
    "\n",
    "p.scatter(source=dataset.datasource, x='x1', y='y1', fill_color='target_color',\n",
    "          line_color='target_color', size='size', name=\"wrong_scatter\",\n",
    "          nonselection_fill_alpha=0.0, nonselection_line_alpha=0.0, marker='cross',\n",
    "          view=CDSView(source=dataset.datasource, filters=[dataset.wrong_filter]))\n",
    "\n",
    "# minimalistic style\n",
    "p.axis.visible = False\n",
    "p.xgrid.visible = False\n",
    "p.ygrid.visible = False\n",
    "#     p.outline_line_color = None\n",
    "p.toolbar_location = \"right\"\n",
    "p.toolbar.logo = None\n",
    "p.toolbar.active_scroll = p.select_one(WheelZoomTool)\n",
    "\n",
    "#pn.pane.Bokeh(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4cb3d-a65a-4b78-8d0c-a1f450c8790a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsets = ['breast', 'diabetes', 'robot24', 'shuttle']\n",
    "max_d = np.zeros(len(dsets))\n",
    "avg_L1_diff = np.zeros((len(dsets),4))\n",
    "parity_rmse = np.zeros((len(dsets),4))\n",
    "corr = np.zeros((len(dsets),4))\n",
    "for k, ds in enumerate(dsets):\n",
    "    print(ds)\n",
    "    for i, key in enumerate(['iLAMP', 'NNInv', 'PCA', 'PCA_filtered']):\n",
    "        for network in ['TensorFlow100']:\n",
    "            x = np.load('benchmarks/'+ds+'/dists_L1_'+ds+'_TensorFlow100.npy')\n",
    "            y = np.load('benchmarks/'+ds+'/IMG_cfs_diff_L1_'+ds+'_'+network+'_'+key+'.npy')\n",
    "            mask = np.argwhere(x != 0).reshape(-1)\n",
    "            print(\"{:.2f}\".format(np.mean(y[mask])), 'average L1 diff', key, network, )\n",
    "            max_d[k] = max(max_d[k], np.amax(y[mask]))\n",
    "            avg_L1_diff[k,i] = np.mean(y[mask])\n",
    "            parity_rmse[k,i] = mean_squared_error(x, y, squared=False)\n",
    "            corr[k,i] = np.corrcoef(np.load('benchmarks/'+ds+'/dists_L1_'+ds+'_TensorFlow100.npy')[mask], np.load('benchmarks/'+ds+'/IMG_cfs_diff_L1_'+ds+'_'+network+'_'+key+'.npy')[mask])[0,1]\n",
    "            #print('avg pix', key, network, 'L2 diff', np.mean(np.load('benchmarks/'+ds+'/IMG_cfs_pix_diff_L2_'+ds+'_'+network+'_'+key+'.npy')[mask]))\n",
    "            #print('average', key, 'L1 diff', np.load('IMG_cfs_diff_L1_'+ds+'_TensorFlow100_'+key+'.npy')[:10])\n",
    "            #print(key, 'cfs', np.load('IMG_cfs_diabetes_TensorFlow100_'+key+'.npy')[index])\n",
    "            #print(key, 'cfs_back', predictor.pip['scaler'].inverse_transform(np.load('IMG_cfs_diabetes_TensorFlow100_'+key+'.npy')[:1]))\n",
    "    math_cfs = np.load('benchmarks/'+ds+'/dists_L1_'+ds+'_TensorFlow100.npy')\n",
    "    print(\"{:.2f}\".format(np.mean(math_cfs[math_cfs != 0])), 'average L1 diff alibi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9202500f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.dpi']= 150\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "from scipy.stats import linregress\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "fig, axes = plt.subplots(len(dsets), 4, figsize=(10,11), sharey='row')\n",
    "fig.suptitle('Parity plots of L1-distances to nearest counterfactual')\n",
    "x_dat = 'dists_L1' # dists_L1, IMG_cfs_pix_diff_L2, IMG_cfs_diff_L1\n",
    "y_dat = 'IMG_cfs_diff_L1'\n",
    "for k, ds in enumerate(dsets):\n",
    "    for i, key in enumerate(['iLAMP', 'NNInv', 'PCA', 'PCA_filtered']):\n",
    "        for j, network in enumerate(['TensorFlow100']):\n",
    "            if k == 0:\n",
    "                axes[k][i].set_title(key)\n",
    "                if key == 'PCA':\n",
    "                    axes[k][i].set_title('CoFFi')\n",
    "                if key == 'PCA_filtered':\n",
    "                    axes[k][i].set_title('filtered CoFFi')\n",
    "                    \n",
    "            mask = np.argwhere(np.load('benchmarks/'+ds+'/dists_L1_'+ds+'_TensorFlow100.npy') != 0).reshape(-1)\n",
    "            if x_dat == 'dists_L1':\n",
    "                x = np.load('benchmarks/'+ds+'/dists_L1_'+ds+'_TensorFlow100.npy')[mask]\n",
    "                if k == len(dsets)-1:\n",
    "                    axes[k][i].set_xlabel('$d_{optimization}$')#'L1-distance to nearest CF')\n",
    "            elif x_dat == 'IMG_cfs_pix_diff_L2':\n",
    "                x = np.load('benchmarks/'+ds+'/'+x_dat+'_'+ds+'_'+network+'_'+key+'.npy')[mask]\n",
    "                if k == len(dsets)-1:\n",
    "                    axes[k][i].set_xlabel('pixel-distance to nearest shown CF')\n",
    "            elif x_dat == 'IMG_cfs_diff_L1':\n",
    "                x = np.load('benchmarks/'+ds+'/'+x_dat+'_'+ds+'_'+network+'_'+key+'.npy')[mask]\n",
    "                if k == len(dsets)-1:\n",
    "                    axes[k][i].set_xlabel('L1-distance to nearest shown CF')\n",
    "                \n",
    "            if y_dat == 'dists_L1':\n",
    "                y = np.load('benchmarks/'+ds+'/dists_L1_'+ds+'_TensorFlow100.npy')[mask]\n",
    "                axes[k][0].set_ylabel('L1-distance to nearest CF')\n",
    "            elif y_dat == 'IMG_cfs_pix_diff_L2':\n",
    "                y = np.load('benchmarks/'+ds+'/'+y_dat+'_'+ds+'_'+network+'_'+key+'.npy')[mask]\n",
    "                axes[k][0].set_ylabel('pixel-distance to nearest shown CF')\n",
    "            elif y_dat == 'IMG_cfs_diff_L1':\n",
    "                y = np.load('benchmarks/'+ds+'/'+y_dat+'_'+ds+'_'+network+'_'+key+'.npy')[mask]\n",
    "                axes[k][0].set_ylabel('$d_{shown}$')#'L1-distance to nearest shown CF')\n",
    "            \n",
    "            # write parity RMSE compared to cf found through optimization (with alibi)\n",
    "            rmse_str = \"{:.2f}\".format(parity_rmse[k,i])\n",
    "            if np.amin(parity_rmse[k,:3]) == parity_rmse[k,i]:\n",
    "                rmse_str = \"\\mathbf{\"+rmse_str+\"}\"\n",
    "            axes[k][i].text(np.max(x),max_d[k]*0.85,\"$RMSE=\"+rmse_str+\"$\", horizontalalignment='right')\n",
    "            \n",
    "            # write correlation between shown and alibi cf\n",
    "            corr_str = \"{:.2f}\".format(corr[k,i])\n",
    "            if np.amax(corr[k,:3]) == corr[k,i]:\n",
    "                corr_str = \"\\mathbf{\"+corr_str+\"}\"\n",
    "            axes[k][i].text(np.max(x),max_d[k]*0.75,r\"$\\rho=\"+corr_str+\"$\", horizontalalignment='right')\n",
    "            \n",
    "            # write average distance of shown cf\n",
    "            avg_str = \"{:.2f}\".format(avg_L1_diff[k,i])\n",
    "            if np.amin(avg_L1_diff[k,:3]) == avg_L1_diff[k,i]:\n",
    "                avg_str = \"\\mathbf{\"+avg_str+\"}\"\n",
    "            axes[k][i].text(np.max(x),max_d[k]*0.95,r\"$\\overline{d_{shown}}=\"+avg_str+\"$\", horizontalalignment='right')\n",
    "            \n",
    "            x1=np.linspace(0,np.max(x),500)\n",
    "            \n",
    "            gradient, intercept, r_value, p_value, std_err = linregress(x,y)\n",
    "            y3=gradient*x1+intercept\n",
    "            #print('regress error', ((gradient*x+intercept - y)**2).mean())\n",
    "            \n",
    "            gradient, residuals, r_value, _ = np.linalg.lstsq(x.reshape(-1,1),y,rcond=None)\n",
    "            y1=gradient*x1\n",
    "            #print('linear error ', ((gradient*x - y)**2).mean()) ### THIS IS GOOD\n",
    "            \n",
    "            popt, pcov = curve_fit(lambda x,a: a*np.sqrt(x), x, y, p0=1)\n",
    "            y2=popt*np.sqrt(x1)\n",
    "            #popt, pcov = curve_fit(lambda x,a: a*x**2, x, y, p0=1)\n",
    "            #y1=popt*x1**2\n",
    "            #print('non-lin error', ((popt*np.sqrt(x) - y)**2).mean())\n",
    "            \n",
    "            # Define a function (quadratic in our case) to fit the data with.\n",
    "            #def f(B, x):\n",
    "            #    return B[0]*x + B[1]\n",
    "            # Create a RealData object using our initiated data from above.\n",
    "            #data = RealData(x, y)\n",
    "            # Set up ODR with the model and data.\n",
    "            #odr = ODR(data, Model(f), beta0=[0, 0])\n",
    "            # Run the regression.\n",
    "            #out = odr.run()\n",
    "            #y1=out.beta[0]*x1+out.beta[1]\n",
    "            #print(list(out.beta))\n",
    "\n",
    "            #axes[i].plot(x, y, 'o', color='black', alpha=0.2, markersize=2)\n",
    "            axes[k][i].plot(x1,x1,'-k', alpha=0.5)\n",
    "            axes[k][i].plot(x, y, 'o', color='black', alpha=0.2, markersize=3, markeredgewidth=0)\n",
    "            #axes[k][i].plot(x1,y1,'-r')\n",
    "            #axes[i].plot(x1,y2,'-g')\n",
    "            #axes[i].plot(x1,y3,'-b')\n",
    "# Set common labels\n",
    "#fig.text(0.5, 0.04, 'common xlabel', ha='center', va='center')\n",
    "    fig.text(0.01, 1 - (k / (len(dsets)+0.35) + 0.63 / (len(dsets))), ds, ha='center', va='center', rotation='vertical')\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"./benchmarks.png\",dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a075c4-f03c-49ce-a49c-bdafe6450e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#math_dists = np.load('benchmarks/'+ds+'/dists_L1_'+ds+'_TensorFlow100.npy')\n",
    "#print(np.sort(math_dists)[:50])\n",
    "#print(dataset.data.loc[np.where(math_dists < 0.2), 'prob'])\n",
    "\n",
    "for k, ds in enumerate(dsets):\n",
    "    print(ds)\n",
    "    for j, network in enumerate(['TensorFlow100']):\n",
    "        for i, key in enumerate(['NNInv', 'iLAMP', 'PCA', 'PCA_filtered']):\n",
    "            mask = np.argwhere(np.load('benchmarks/'+ds+'/dists_L1_'+ds+'_TensorFlow100.npy') != 0).reshape(-1)\n",
    "            #print('corr  math-CF to pix-diff', network, key, np.corrcoef(np.load('benchmarks/'+params.dataset_name+'/dists_L1_'+ds+'_'+network+'.npy'), np.load('benchmarks/'+params.dataset_name+'/IMG_cfs_pix_diff_L2_'+ds+'_'+network+'_'+key+'.npy'))[0,1])   \n",
    "\n",
    "            #print('corr image-CF to pix-diff', network, key, np.corrcoef(np.load('benchmarks/'+ds+'/IMG_cfs_diff_L1_'+ds+'_'+network+'_'+key+'.npy'), np.load('benchmarks/'+ds+'/IMG_cfs_pix_diff_L2_'+ds+'_'+network+'_'+key+'.npy'))[0,1])\n",
    "            \n",
    "            #cov = np.cov(np.array([np.load('benchmarks/'+ds+'/dists_L1_'+ds+'_TensorFlow100.npy'), np.load('benchmarks/'+ds+'/IMG_cfs_diff_L1_'+ds+'_'+network+'_'+key+'.npy')]))\n",
    "            #print(cov[0,1] / (np.sqrt(cov[0,0])*np.sqrt(cov[1,1])))\n",
    "            \n",
    "            print('corr  math-CF to image-CF', network, key, np.corrcoef(np.load('benchmarks/'+ds+'/dists_L1_'+ds+'_TensorFlow100.npy')[mask], np.load('benchmarks/'+ds+'/IMG_cfs_diff_L1_'+ds+'_'+network+'_'+key+'.npy')[mask])[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f66ec-8408-403f-ba10-9034c4f00442",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xai-test]",
   "language": "python",
   "name": "conda-env-xai-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
