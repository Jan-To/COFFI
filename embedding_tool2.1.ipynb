{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COFFI: \n",
    "\n",
    "A visual analytics system for the exploration of black-box-classifiers concentrating on decision boundaries and counterfactuals.\n",
    "\n",
    "Execute:\n",
    "```\n",
    "panel serve --show embedding_tool2.1.ipynb --port 8080 --dev\n",
    "```\n",
    "\n",
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print events\n",
    "from __future__ import print_function\n",
    "\n",
    "css = '''\n",
    ".bk.header-box {\n",
    "  background: #f0f0f0;\n",
    "  //border-radius: 5px;\n",
    "}\n",
    "  \n",
    ".bk.header {\n",
    "  top: 0px !important;\n",
    "}\n",
    "\n",
    ".bk.featureList {\n",
    "    //background: #ffdd00;\n",
    "    \n",
    "}\n",
    "\n",
    ".history-box {\n",
    "  overflow: auto;\n",
    "}\n",
    "\n",
    ".bk.panel-test-box {\n",
    "  border-bottom: 1px #f0f0f0 solid;\n",
    "}\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI library\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "import seaborn as sns\n",
    "import time\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "\n",
    "from bokeh.plotting import figure\n",
    "from bokeh.palettes import Category10, PiYG, linear_palette\n",
    "from bokeh.models import ColumnDataSource, TapTool, CDSView, IndexFilter, BooleanFilter, CustomJS\n",
    "from bokeh.transform import linear_cmap\n",
    "from bokeh.events import Event, Tap, LODEnd\n",
    "from bokeh.models.widgets.tables import NumberFormatter, DataTable\n",
    "from bokeh.models.widgets import Div\n",
    "from bokeh.embed import file_html, json_item\n",
    "from bokeh.io.export import get_screenshot_as_png\n",
    "\n",
    "import param\n",
    "import panel as pn\n",
    "from panel.io import save\n",
    "\n",
    "import json\n",
    "\n",
    "pn.extension(raw_css=[css])\n",
    "\n",
    "\n",
    "# local files\n",
    "from embedding_util2_1 import load_dataset, partial_dependence, plot_horizons, \\\n",
    "            embedding_view, update_embedding, update_horizons, update_table,   \\\n",
    "            update_df, rgb_to_hex, Shifter, SVD, update_horizon_lines,         \\\n",
    "            non_linear_view, update_non_linear_view, colorbar_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params():\n",
    "    total_width = 1400\n",
    "    total_height = 500\n",
    "    bot_height = 300\n",
    "    header_height = 32\n",
    "    emb_width = 500\n",
    "    fea_width = 160\n",
    "    tbl_width = 100\n",
    "    hor_width = total_width-(emb_width+fea_width+tbl_width)\n",
    "    red_res = 500\n",
    "    pdp_res = 50\n",
    "    dot_size = 6\n",
    "    \n",
    "    dataset_name = 'diabetes'\n",
    "    neighbors = 100\n",
    "    classifier = 'RandomForest'\n",
    "    \n",
    "    palette = []\n",
    "    cm = None\n",
    "    num_colors = 8\n",
    "    sat_palette = []\n",
    "    sat_cm = None\n",
    "    \n",
    "    def __init__(self):\n",
    "        sat_color_list = ['#d95f02','#1b9e77','#7570b3','#e6ab02','#66a61e','#e7298a','#a6761d','#666666'] #1\n",
    "#        sat_color_list = ['#ff6023','#36c296','#5a7dcc','#e7298a','#66a61e','#e6ab02','#a6761d','#666666'] #1.1\n",
    "#         sat_color_list = ['#5da5da','#faa43a','#60bd68','#f17cb0','#b2912f','#b276b2','#decf3f','#f15854'] #2\n",
    "        color_list = ['#fc8d62','#66c2a5','#8da0cb','#ffd92f','#a6d854','#e78ac3','#e5c494','#b3b3b3'] #1\n",
    "#         color_list = [rgb_to_hex(sns.light_palette(c, n_colors=6)[3]) for c in sat_color_list]\n",
    "        num_colors = len(color_list)\n",
    "        for i in range(num_colors):\n",
    "            self.palette += [rgb_to_hex(c) for c in sns.light_palette(color_list[i], n_colors=6)][1:]\n",
    "        self.sat_palette = sat_color_list\n",
    "        self.cm = ListedColormap(colors=self.palette)\n",
    "        self.sat_cm = ListedColormap(colors=self.sat_palette)\n",
    "            \n",
    "params = Params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset():\n",
    "    name = ''\n",
    "    data = None \n",
    "    datasource = ColumnDataSource()\n",
    "    view_filter = IndexFilter()\n",
    "    wrong_filter = IndexFilter()\n",
    "    view = CDSView()\n",
    "    labels = None\n",
    "    features = []\n",
    "    selected_features = []\n",
    "    classes = None\n",
    "    categories = None\n",
    "    trees = []\n",
    "    \n",
    "    bounds = []\n",
    "    point = []\n",
    "    \n",
    "    def update(self, name):\n",
    "        self.name = name\n",
    "        self.data,self.labels,self.features,self.classes,_,self.categories = load_dataset(self.name)\n",
    "        self.selected_features = self.features\n",
    "        \n",
    "        self.data['color']      = ['#444444']*len(self.data.index)\n",
    "        self.data['sat_color']  = ['#444444']*len(self.data.index)\n",
    "        self.data['line_color'] = ['#444444']*len(self.data.index)\n",
    "        self.data['size'] = [params.dot_size]*len(self.data.index)\n",
    "        self.data['x']  = [0]*len(self.data.index)\n",
    "        self.data['y']  = [0]*len(self.data.index)\n",
    "        self.data['x1'] = [0]*len(self.data.index)\n",
    "        self.data['y1'] = [0]*len(self.data.index)\n",
    "        self.datasource.selected.indices = [0] \n",
    "        self.datasource.selected.indices = []\n",
    "        self.datasource.data = self.data\n",
    "        self.view_filter.indices  = list(self.data.index)\n",
    "        self.wrong_filter.indices = list(self.data.index)\n",
    "        self.view = CDSView(source=self.datasource, filters=[self.view_filter])\n",
    "        \n",
    "#         self.bounds = [dataset.data[dataset.features].quantile(0.05).to_numpy(), dataset.data[dataset.features].quantile(0.95).to_numpy()]\n",
    "        self.bounds = [dataset.data[dataset.features].min().to_numpy(), dataset.data[dataset.features].max().to_numpy()]\n",
    "        self.point = dataset.data[dataset.features].mean()\n",
    "        print(\"--- data loaded\", self.data.shape)\n",
    "    \n",
    "dataset = Dataset()\n",
    "dataset.update(params.dataset_name)\n",
    "\n",
    "class Point():\n",
    "    def __init__(self):\n",
    "        self.point = ColumnDataSource({})\n",
    "        self.line = ColumnDataSource({})\n",
    "        self.text = ColumnDataSource({})\n",
    "    \n",
    "    def reset(self):\n",
    "        self.point.data = {}\n",
    "        self.line.data = {}\n",
    "        self.text.data = {}\n",
    "        \n",
    "cf, pt = Point(), Point()\n",
    "\n",
    "class Predictor():\n",
    "    pip = None\n",
    "    \n",
    "    def update_pip(self, classifier_type):\n",
    "        if classifier_type == \"RandomForest\":\n",
    "            self.pip = Pipeline([('scaler', StandardScaler()), \n",
    "                                 ('classificator', RandomForestClassifier(\n",
    "                                      random_state=1, min_samples_split = 4, min_samples_leaf = 3))])\n",
    "        elif classifier_type == \"NeuralNetwork\":\n",
    "            self.pip = Pipeline([('scaler', StandardScaler()), \n",
    "                                 ('classificator', MLPClassifier(hidden_layer_sizes=(100,100,100), \n",
    "                                                                 max_iter=5000, random_state=1))])\n",
    "        else: print(\"unkown classifier type:\", classifier_type)\n",
    "        \n",
    "    def update_data(self, dataset, params):\n",
    "        # train predictor on dataset\n",
    "        #from sklearn.model_selection import train_test_split\n",
    "        #X_train, X_test, y_train, y_test = train_test_split(dataset.data[dataset.features], dataset.labels, test_size=0.2, random_state=42)\n",
    "        #self.pip.fit(X_train, y_train)\n",
    "        #score = self.pip['classificator'].score(self.pip['scaler'].transform(X_test), y_test)\n",
    "        #print('score: ', score)\n",
    "        self.pip.fit(dataset.data[dataset.features], dataset.labels)\n",
    "        # save predicitons in dataset\n",
    "        dataset.data['prob'] = self.pip.predict_proba(dataset.data[dataset.features]).tolist()\n",
    "        dataset.data['maxprob'] = np.max(np.array(dataset.data['prob'].tolist()), axis=1)\n",
    "        dataset.datasource.data['prob'] = dataset.data['prob']\n",
    "        dataset.datasource.data['maxprob'] = dataset.data['maxprob']\n",
    "        # compute colors of dataset\n",
    "        dataset.data['most_prob_class'] = np.argmax(np.array(dataset.data['prob'].tolist()), axis=1)\n",
    "        colorvalues = (np.clip(dataset.data['maxprob'].to_numpy(),0.51,0.99) - 0.5) * 2 + dataset.data['most_prob_class'].to_numpy()\n",
    "        dataset.data['color'] = [rgb_to_hex(params.cm(i/params.num_colors)) for i in colorvalues]\n",
    "        dataset.data['sat_color'] = [params.sat_palette[i] for i in dataset.data['most_prob_class'].to_numpy()]\n",
    "        dataset.data['line_color'] = dataset.data['sat_color'].copy()\n",
    "        dataset.datasource.data['color'] = dataset.data['color']\n",
    "        dataset.datasource.data['sat_color'] = dataset.data['sat_color']\n",
    "        dataset.datasource.data['line_color'] = dataset.data['sat_color'].copy()\n",
    "        \n",
    "        # compute wrongly predicted indices\n",
    "        dataset.wrong_filter.indices = np.where(np.not_equal(dataset.data['most_prob_class'].to_numpy(), dataset.labels))[0].tolist()\n",
    "#         dataset.datasource.data['target_color'] = np.where(\n",
    "#             dataset.data['most_prob_class'].to_numpy() != dataset.labels,\n",
    "#             [rgb_to_hex(params.sat_cm(i/params.num_colors)) for i in (dataset.labels+0.99)], \n",
    "#             np.full(len(dataset.data),'#333333'))\n",
    "        dataset.datasource.data['target_color'] = [params.sat_palette[i] for i in dataset.labels]\n",
    "        \n",
    "        # create nearest neighbor search structure per class\n",
    "        dataset.trees = []\n",
    "        for i in range(len(dataset.classes)):\n",
    "            index_map = np.where(dataset.data['most_prob_class'].to_numpy() == i)[0]\n",
    "            class_data = dataset.data.loc[index_map,dataset.features]\n",
    "            scaled_class_data = self.pip['scaler'].transform(class_data)\n",
    "            dataset.trees.append( (index_map, BallTree(scaled_class_data)) )\n",
    "\n",
    "predictor = Predictor()\n",
    "predictor.update_pip(params.classifier)\n",
    "predictor.update_data(dataset, params)\n",
    "\n",
    "# class LL():\n",
    "#     svm = None\n",
    "#     svd = None\n",
    "#     components_ = None\n",
    "#     V = np.zeros(0)\n",
    "    \n",
    "#     def __init__(self):\n",
    "#         self.svm = LinearSVC(random_state=0)\n",
    "#         self.svd = SVD()\n",
    "    \n",
    "#     def fit(self, X, y, sample_weight=None):\n",
    "#         self.svm.fit(X, y)\n",
    "#         self.svd.fit(X)\n",
    "#         self.components_ = np.append(self.svm.coef_/np.linalg.norm(self.svm.coef_), [[0.01]*len(self.svm.coef_[0])], axis=0)\n",
    "# #                                      [self.svd.components_[0]], axis=0)\n",
    "        \n",
    "#     def transform(self, X):\n",
    "#         X = np.dot(X, self.components_.T)\n",
    "#         return X\n",
    "    \n",
    "#     def inverse_transform(self, X):\n",
    "#         X = np.dot(X, self.components_)\n",
    "#         return X\n",
    "        \n",
    "    \n",
    "class Embedding():\n",
    "    emb = Pipeline([('shifter', Shifter()), ('scaler', StandardScaler()), ('pca', SVD())])\n",
    "    zoom = \"Global\"\n",
    "    focus = \"Mean\"\n",
    "    nn_ind = []\n",
    "    \n",
    "    def compute(self, dataset):\n",
    "        nn = len(dataset.datasource.selected.indices)\n",
    "        if nn > 1:\n",
    "            self.nn_ind = dataset.datasource.selected.indices\n",
    "        else:\n",
    "            self.nn_ind = dataset.data.index.tolist()\n",
    "            nn = len(self.nn_ind)\n",
    "               \n",
    "        train_set = dataset.data.loc[self.nn_ind][dataset.selected_features]\n",
    "        #tic = time.perf_counter()\n",
    "        self.emb.fit(train_set)\n",
    "        pp = self.emb.transform(dataset.data[dataset.selected_features])\n",
    "        #toc = time.perf_counter()\n",
    "        \n",
    "        #print(\"Compute plane:\",toc-tic,\"s\")\n",
    "        \n",
    "        dataset.data['x'] = pp[:,0]\n",
    "        dataset.data['y'] = pp[:,1]\n",
    "        dataset.datasource.data['x'] = pp[:,0]\n",
    "        dataset.datasource.data['y'] = pp[:,1]\n",
    "        \n",
    "        shift_bounds = [[dataset.bounds[j][i] for i,f in enumerate(dataset.features) if (f in dataset.selected_features)] for j in [0,1]]\n",
    "        self.emb['shifter'].set_bounds(shift_bounds) \n",
    "        if dataset.point.name != None:\n",
    "            # shift plane to focus point\n",
    "            point = dataset.point[dataset.selected_features]\n",
    "            shift = point.values.astype('float64') - self.emb.inverse_transform(self.emb.transform([point]))[0]\n",
    "            self.emb['shifter'].set_by(shift)\n",
    "        \n",
    "        self.focus = str(dataset.point.name) if dataset.point.name != None else \"Mean\"\n",
    "        self.zoom = str(nn)+\" samples\"\n",
    "            \n",
    "\n",
    "embedding = Embedding()\n",
    "embedding.compute(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GUI parameters & widgets\n",
    "dataset_selector = pn.widgets.Select(name='', options=[\"iris\",\"income\",\"diabetes\",\"breast\",\"heart-failure\",\"shuttle\",\"robot24\",\"robot4\"], value=params.dataset_name, width=100)\n",
    "predictor_selector = pn.widgets.Select(name='', value=params.classifier, options=[\"RandomForest\",\"NeuralNetwork\"], width=120)\n",
    "neighbors = pn.widgets.TextInput(name='', placeholder=str(params.neighbors), value=str(params.neighbors), width=50)\n",
    "zoom = pn.widgets.Button(name='Embed: '+embedding.zoom, width=120)\n",
    "focus = pn.widgets.Button(name='Shift: '+embedding.focus, width=90)\n",
    "select_features = pn.widgets.Button(name='Filter Features', width=90)\n",
    "reset = pn.widgets.Button(name='Reset', width=60)\n",
    "average = pn.widgets.Toggle(name='Average', width=60, value=False)\n",
    "nl_select = pn.widgets.Select(name='', value='UMAP', options=['UMAP', 't-SNE'], width=180)\n",
    "nl_neighbors = pn.widgets.IntSlider(name='Neighborhood size', start=5, end=200, step=5, value=20, width=180)\n",
    "nl_feats = pn.widgets.Checkbox(name='use only selected features', value=False, width=180, height=30)\n",
    "feature_selector = pn.widgets.CheckBoxGroup(name='', value=dataset.features, options=dataset.features, \n",
    "                                            inline=False, width=120, css_classes=[\"featureList\"], \n",
    "                                            max_height=params.total_height-10)\n",
    "css_code = pn.pane.HTML(f'''\n",
    "            <style>\n",
    "            .bk label {{\n",
    "                line-height: {params.total_height / (13*len(feature_selector.options))} !important\n",
    "            }}\n",
    "            </style>\n",
    "            ''')\n",
    "emb_header = pn.pane.Markdown(\"\", style={\n",
    "        'width': \"700px\", 'height': str(params.header_height)+\"px\", \"text-align\": \"left\"})\n",
    "#for i, c in enumerate(dataset.classes):\n",
    "#    emb_header.object += \"![color](https://placehold.it/15/\" + params.palette[i*5+4][1:] \\\n",
    "#        + \"/000000?text=+) \" + c + \"  \" \n",
    "    \n",
    "# projection view\n",
    "detail_p = embedding_view(params, dataset, embedding, predictor.pip, average.value, cf.point)\n",
    "\n",
    "# non-linear view\n",
    "nonlinear = non_linear_view(params, dataset, predictor.pip)\n",
    "\n",
    "# importance table\n",
    "table_source = ColumnDataSource()\n",
    "table = DataTable(source=table_source, editable=False, index_position=None, \n",
    "                  height=params.total_height+17, width=params.tbl_width)\n",
    "update_table(table, table_source, dataset, predictor.pip, embedding, params)\n",
    "\n",
    "# PDP horizons\n",
    "horizons = plot_horizons(predictor.pip.predict_proba, dataset, params, pt, cf)\n",
    "\n",
    "def update_line_source(src, point, opposite):\n",
    "    src.line.data = {**{key: [val,val] for (key,val) in zip(dataset.features, point)}, \"y\":[0.0,0.25]}\n",
    "    src.text.data = {**{key: [val] for (key,val) in zip(dataset.features, point)}, \"y\":[0.25],\n",
    "                     **{key+\"_label\": ['{:3.1f}'.format(val)] for (key,val) in zip(dataset.features, point)},\n",
    "                     **{key+\"_align\": ['right'] if p < o else ['left'] for key, p, o in zip(dataset.features, point, opposite)},\n",
    "                     **{key+\"_x_offset\": [-2] if p < o else [2] for key, p, o in zip(dataset.features, point, opposite)}}\n",
    "\n",
    "update_line_source(pt, dataset.point[dataset.features], dataset.point[dataset.features])\n",
    "cf.line.data = {}\n",
    "# update_horizon_lines(horizons, dataset.features, dataset.point, name=\"point\")\n",
    "\n",
    "# history\n",
    "history = pn.Row(width=params.emb_width, max_width=params.emb_width+30, \n",
    "                 height=params.bot_height, max_height=params.bot_height, \n",
    "                 css_classes=['history-box'])\n",
    "\n",
    "# data table\n",
    "df_widget = DataTable(source=dataset.datasource, autosize_mode='fit_columns', \n",
    "                      height=params.bot_height+20, width=params.total_width-params.emb_width+10,\n",
    "                      view=dataset.view)\n",
    "update_df(df_widget, dataset, params)\n",
    "\n",
    "# color bar\n",
    "colorbar = colorbar_view(params, dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-----------------------------------------------------------------------------------------------\n",
    "# when choosing new dataset from dropdown\n",
    "\n",
    "def onDatasetChanged(event):\n",
    "    if event.type == 'changed':\n",
    "        dataset.update(event.new)\n",
    "        \n",
    "        # reset lines & selection\n",
    "        cf.reset()\n",
    "\n",
    "        # re-train predictor\n",
    "        e = Event()\n",
    "        e.type = \"changed\"\n",
    "        onPredictorChanged(e)\n",
    "        \n",
    "        # update non-linear embedding\n",
    "        update_non_linear_view(nonlinear.object, dataset, predictor.pip, nl_select.value, nl_neighbors.value)\n",
    "        \n",
    "        # update feature selector\n",
    "        feature_selector.options = dataset.features\n",
    "        feature_selector.value = dataset.features\n",
    "        label_height = params.total_height / (13*len(feature_selector.options))\n",
    "        css_code.object = f'''\n",
    "            <style>\n",
    "            .bk label {{\n",
    "                line-height: {label_height} !important\n",
    "            }}\n",
    "            </style>\n",
    "            '''\n",
    "        \n",
    "        # update table\n",
    "        update_table(table, table_source, dataset, predictor.pip, embedding, params)\n",
    "        \n",
    "        # update header\n",
    "        #emb_header.object = \"\"\n",
    "        #for i, c in enumerate(dataset.classes):\n",
    "        #    emb_header.object += \"![color](https://placehold.it/15/\" + params.palette[i*5+4][1:] \\\n",
    "        #                + \"/000000?text=+) \" + c + \"  \" \n",
    "            \n",
    "        # update colorbar\n",
    "        colorbar.object = colorbar_view(params, dataset).object    \n",
    "        \n",
    "        # deselect and select all points\n",
    "        dataset.datasource.selected.indices = [i for i in range(dataset.data.shape[0])]\n",
    "        dataset.datasource.selected.indices = []\n",
    "        \n",
    "        history.clear()\n",
    "        \n",
    "dataset_selector.param.watch(onDatasetChanged, 'value')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# when choosing new predictor from dropdown\n",
    "\n",
    "def onPredictorChanged(event):\n",
    "    if event.type == 'changed':\n",
    "        # update predictor\n",
    "        predictor.update_pip(predictor_selector.value)\n",
    "        predictor.update_data(dataset, params)\n",
    "        \n",
    "        # update selection table\n",
    "        update_df(df_widget, dataset, params)\n",
    "        \n",
    "        # project data\n",
    "        embedding.compute(dataset)\n",
    "        \n",
    "        # update embedding view\n",
    "        update_embedding(detail_p.object, params, dataset, embedding, predictor.pip, average.value)\n",
    "        \n",
    "        # update table\n",
    "        update_table(table, table_source, dataset, predictor.pip, embedding, params)\n",
    "        \n",
    "        # update horizons\n",
    "        horizons.objects = plot_horizons(predictor.pip.predict_proba, dataset, params, pt, cf).objects\n",
    "        cf_temp = [cf.point.data[f][0] for f in dataset.features] if cf.point.data != {} else dataset.point[dataset.features]\n",
    "        update_line_source(pt, dataset.point[dataset.features], cf_temp)\n",
    "        \n",
    "        update_horizons(horizons, dataset, params)\n",
    "        \n",
    "predictor_selector.param.watch(onPredictorChanged, 'value')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# tapping in the embedding view\n",
    "\n",
    "def onTap(event):\n",
    "    # compute tapped point through inverse\n",
    "    cf_pt = embedding.emb.inverse_transform([[event.x, event.y]])[0]\n",
    "    for i, f in enumerate(dataset.features):\n",
    "        if f not in dataset.selected_features:\n",
    "            cf_pt = np.insert(cf_pt, i, dataset.point[f])\n",
    "    # update datasource accordingly\n",
    "    update_line_source(cf, cf_pt, dataset.point[dataset.features])\n",
    "    cf.point.data   = {key: [val] for (key,val) in zip(dataset.features+[\"x\",\"y\"], np.append(cf_pt,[event.x,event.y]))}\n",
    "    \n",
    "    # tapped on data point\n",
    "    if len(dataset.datasource.selected.indices) != 0:\n",
    "        p = dataset.datasource.selected.indices[0]\n",
    "        dataset.point = dataset.data.loc[p].T\n",
    "        # highlight tapped point in size & border (take first one in selection)\n",
    "        new_size = [params.dot_size]*len(dataset.data)\n",
    "        new_size[p] = 13\n",
    "        dataset.datasource.data['size'] = new_size\n",
    "        new_line = dataset.data['sat_color']\n",
    "        new_line[p] = 'black'\n",
    "        dataset.datasource.data['line_color'] = new_line\n",
    "    # update horizon lines\n",
    "    cf_temp = [cf.point.data[f][0] for f in dataset.features] if cf.point.data != {} else dataset.point[dataset.features]\n",
    "    update_line_source(pt, dataset.point[dataset.features], cf_temp)\n",
    "    \n",
    "detail_p.object.on_event(Tap, onTap)\n",
    "nonlinear.object.on_event(Tap, onTap)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# Selecting one point in the embedding\n",
    "\n",
    "def onSelectionChange(attr, old, new):\n",
    "    print('selection changed', 'old', old, 'new', new, 'indices', dataset.datasource.selected.indices)\n",
    "    if len(new) == 0 and len(old) != 1: \n",
    "        dataset.datasource.selected.indices = old\n",
    "    \n",
    "dataset.datasource.selected.on_change('indices', onSelectionChange)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# change the feature selection\n",
    "\n",
    "def onFeatureSelectionChanged(event):\n",
    "    if len(event.new) < 2 or len(event.old) < 1: return\n",
    "    add_history()\n",
    "    \n",
    "    dataset.selected_features = [x for _,x in sorted([(dataset.features.index(sf),sf) for sf in event.new if sf in dataset.features], key=lambda tup: tup[0])]\n",
    "    \n",
    "    # recompute embedding\n",
    "    embedding.compute(dataset)\n",
    "    cf.reset()\n",
    "        \n",
    "    # update embedding view\n",
    "    update_embedding(detail_p.object, params, dataset, embedding, predictor.pip, average.value)\n",
    "    \n",
    "    # update pdp\n",
    "    update_horizons(horizons, dataset, params)\n",
    "    \n",
    "    # update table\n",
    "    update_table(table, table_source, dataset, predictor.pip, embedding, params, full=False)\n",
    "    \n",
    "    # update non-linear if necessary\n",
    "    if nl_feats.value : change_non_linear()\n",
    "    \n",
    "feature_selector.param.watch(onFeatureSelectionChanged, 'value')\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# change the feature selection\n",
    "\n",
    "def onSelectImportantFeatures(event=None):\n",
    "    important_indices = np.where(table_source.data[\"Permutation Importance\"] > 100.0/len(dataset.features))[0]\n",
    "    feature_selector.value = np.array(dataset.features)[important_indices].tolist()\n",
    "\n",
    "select_features.on_click(onSelectImportantFeatures)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------------\n",
    "# add current embedding & parameters to history\n",
    "\n",
    "def add_history(flip_avg=False):\n",
    "    return\n",
    "    im = get_screenshot_as_png(detail_p.object)\n",
    "    history.insert(0, pn.Column(pn.pane.PNG(im, width=int(params.total_height/2.5)),\n",
    "                                pn.pane.Markdown(\"Embed: \"+embedding.zoom \\\n",
    "                                                 +\"<br />Shift: \"+embedding.focus))) \\\n",
    "#                                                  +\"<br />Plain: \"+(\"Averaged\" if average.value != flip_avg else \"Exact\"))))\n",
    "    \n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# Shift base of embedding to last selected point\n",
    "def onShift(event=None, add_hist=True):\n",
    "    if add_hist: add_history()\n",
    "    \n",
    "    # project data\n",
    "    embedding.compute(dataset)\n",
    "    zoom.name = \"Embed: \"+embedding.zoom\n",
    "    focus.name = \"Shift: \"+embedding.focus\n",
    "    cf.reset()\n",
    "    \n",
    "    # update embedding view\n",
    "    update_embedding(detail_p.object, params, dataset, embedding, predictor.pip, average.value)\n",
    "    \n",
    "    # update table\n",
    "    update_table(table, table_source, dataset, predictor.pip, embedding, params, full=False)\n",
    "\n",
    "    # update horizons\n",
    "    update_line_source(pt, dataset.point[dataset.features], dataset.point[dataset.features])\n",
    "    horizons.objects = plot_horizons(predictor.pip.predict_proba, dataset, params, pt, cf).objects\n",
    "    \n",
    "focus.on_click(onShift)\n",
    "\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# Rotate embedding according to PCA of nearest neighbors (same class + other class) of last selected point\n",
    "\n",
    "def zoom_view(event):\n",
    "    add_history()\n",
    "    \n",
    "    # set embedding anchor\n",
    "    selection = dataset.datasource.selected.indices\n",
    "    # zoom without selection -> reset zoom / zoom all\n",
    "    if len(selection) is 0: \n",
    "        dataset.point = dataset.data[dataset.features].mean()\n",
    "    # zoom with point selected -> zoom to point and its neighbors\n",
    "    elif len(selection) < 3:\n",
    "        on_select_nn()\n",
    "        # focus selected point (take first one in selection)\n",
    "        p = dataset.datasource.selected.indices[0]\n",
    "        dataset.point = dataset.data.loc[p].T\n",
    "        # highlight tapped point in size & border \n",
    "        new_size = [params.dot_size]*len(dataset.data)\n",
    "        new_size[p] = 13\n",
    "        dataset.datasource.data['size'] = new_size\n",
    "        new_line = dataset.data['sat_color']\n",
    "        new_line[p] = 'black'\n",
    "        dataset.datasource.data['line_color'] = new_line\n",
    "    # zoom with group of points selected without point focused -> zoom average of group\n",
    "    elif dataset.point.name is None:\n",
    "        dataset.point = dataset.data.loc[selection][dataset.features].mean()\n",
    "    # else zoom to focused point with regard to selected group\n",
    "        \n",
    "    # update horizons\n",
    "    update_line_source(pt, dataset.point[dataset.features], dataset.point[dataset.features])\n",
    "    horizons.objects = plot_horizons(predictor.pip.predict_proba, dataset, params, pt, cf).objects\n",
    "    \n",
    "    # project data\n",
    "    embedding.compute(dataset)\n",
    "    zoom.name = \"Embed: \"+embedding.zoom\n",
    "    focus.name = \"Shift: \"+embedding.focus\n",
    "    cf.reset()\n",
    "    \n",
    "    # update embedding view\n",
    "    update_embedding(detail_p.object, params, dataset, embedding, predictor.pip, average.value)\n",
    "    \n",
    "    # update table\n",
    "    update_table(table, table_source, dataset, predictor.pip, embedding, params)\n",
    "    \n",
    "zoom.on_click(zoom_view)\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# Switch between average and exact embedding\n",
    "def switch_average(event):\n",
    "    add_history(flip_avg=True)\n",
    "    \n",
    "    # update embedding view\n",
    "    update_embedding(detail_p.object, params, dataset, embedding.emb, predictor.pip, event.new)\n",
    "\n",
    "    # update horizons\n",
    "#     horizons.objects = plot_horizons(predictor.pip.predict_proba, dataset, params).objects\n",
    "#     update_horizon_lines(horizons, dataset.features, dataset.point, name=\"point\")\n",
    "    \n",
    "average.param.watch(switch_average, 'value')\n",
    "\n",
    "#-----------------------------------------------------------------------------------------------\n",
    "# Reset embedding to default (Global + Mean)\n",
    "\n",
    "def reset_view(event):\n",
    "    add_history()\n",
    "    if len(dataset.datasource.selected.indices) > 1:\n",
    "        dataset.datasource.selected.indices = [dataset.datasource.selected.indices[0]] \n",
    "    dataset.datasource.selected.indices = []\n",
    "    dataset.point = dataset.data[dataset.features].mean()\n",
    "    dataset.datasource.data['size'] = [params.dot_size]*len(dataset.data)\n",
    "    onShift(add_hist=False)\n",
    "    \n",
    "reset.on_click(reset_view)\n",
    "\n",
    "def change_non_linear(event=None):\n",
    "    update_non_linear_view(nonlinear.object, dataset, predictor.pip, nl_select.value, nl_neighbors.value, nl_feats.value)\n",
    "    \n",
    "nl_select.param.watch(change_non_linear, 'value')\n",
    "nl_neighbors.param.watch(change_non_linear, 'value')\n",
    "nl_feats.param.watch(change_non_linear, 'value')\n",
    "\n",
    "def resample_emb(event):\n",
    "    plot_range = [detail_p.object.x_range.start, detail_p.object.x_range.end,\n",
    "                  detail_p.object.y_range.start, detail_p.object.y_range.end]\n",
    "    update_embedding(detail_p.object, params, dataset, embedding, predictor.pip, average.value, plot_range)\n",
    "    \n",
    "detail_p.object.on_event(LODEnd, resample_emb)\n",
    "\n",
    "def on_select_nn(event=None):\n",
    "    # get current focused point\n",
    "    p = dataset.point\n",
    "    num_nn = int(neighbors.value)\n",
    "    # find nearest neighbors distributed: 1/2 own class, 1/2 other classes\n",
    "    own_class = np.argmax(p['prob']) if p.name != None else np.argmax(predictor.pip.predict_proba([p[dataset.features]])[0])\n",
    "    own_nn = np.array([])\n",
    "    other_dist = np.full((len(dataset.classes), num_nn//2), np.inf)\n",
    "    other_nn   = np.full((len(dataset.classes), num_nn//2), -1)\n",
    "    for i in range(len(dataset.classes)):\n",
    "        dist, ind = dataset.trees[i][1].query(\n",
    "            predictor.pip['scaler'].transform([p[dataset.features]]), k=min(num_nn//2,len(dataset.trees[i][0])))\n",
    "        if i == own_class:\n",
    "            own_nn = np.append(own_nn, dataset.trees[i][0][ind])\n",
    "        else:\n",
    "            other_dist[i,:dist.shape[1]] = dist[0]\n",
    "            other_nn[i,:dist.shape[1]] = dataset.trees[i][0][ind[0]]\n",
    "    other_nn = other_nn[np.unravel_index(np.argsort(other_dist, axis=None), other_dist.shape)]\n",
    "    other_nn = np.delete(other_nn, np.where(other_nn == -1))\n",
    "    other_nn = other_nn[:min(num_nn//2, len(other_nn))]\n",
    "    nn = np.append(own_nn, other_nn).astype(int).tolist()\n",
    "    dataset.datasource.selected.indices = nn\n",
    "\n",
    "\n",
    "filter_callback = CustomJS(args=dict(src=dataset.datasource, filter=dataset.view_filter, nn=embedding.nn_ind), code='''\n",
    "    if (src.selected.indices.length > 0) {\n",
    "        filter.indices = src.selected.indices\n",
    "    } else {\n",
    "        filter.indices = nn\n",
    "    }\n",
    "    src.change.emit()\n",
    "''')\n",
    "\n",
    "dataset.datasource.selected.js_on_change('indices', filter_callback)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "click table\n",
    "https://stackoverflow.com/questions/55403853/how-to-get-a-list-of-bokeh-widget-events-and-attributes-which-can-be-used-to-tr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the GUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffi = pn.Column(pn.Row(pn.pane.Markdown('''## CoFFi''', style={'height':'31px'}, css_classes=['header']), \n",
    "                         pn.Spacer(width=50),\n",
    "                         pn.pane.Markdown(\"#### Dataset\", style={'height':'31px'}), \n",
    "                         dataset_selector,\n",
    "                         pn.pane.Markdown(\"#### Classifier\", style={'height':'31px'}),\n",
    "                         predictor_selector,\n",
    "                         pn.pane.Markdown(\"#### Neighbors\", style={'height':'31px'}),\n",
    "                         neighbors,\n",
    "                         emb_header, \n",
    "                         reset,\n",
    "                         css_classes=['header-box']), \n",
    "                  pn.Row(pn.Column(pn.Row(pn.pane.Markdown(\"#### Embedding View\", style={'height':str(params.header_height)+'px'}),\n",
    "                                          pn.Spacer(width=2), focus, zoom, pn.Spacer(width=3), select_features), detail_p),\n",
    "#                          pn.Column(pn.pane.Markdown(\"#### Legend\", style={'height':str(params.header_height)+'px'}), colorbar),\n",
    "                         pn.Column(pn.Spacer(height=7), table),\n",
    "                         pn.Column(pn.pane.Markdown(\"#### Features\", style={'width': str(params.fea_width)+\"px\", 'height': str(params.header_height)+\"px\"}),\n",
    "                                    feature_selector),\n",
    "                         pn.Column(pn.pane.Markdown(\"#### Partial Dependence\", style={'width': str(params.hor_width)+\"px\", 'height': str(params.header_height)+\"px\"}),\n",
    "                                    horizons),\n",
    "                         css_classes=['panel-test-box']),\n",
    "                  pn.Row(pn.Tabs(('Topology View', pn.Row(pn.Column(nl_select, nl_neighbors, nl_feats, pn.Spacer(height=5), colorbar), nonlinear)), \n",
    "                                 ('Inspection History', history)), df_widget, css_code))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coffi.servable(\"COFFI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:xai]",
   "language": "python",
   "name": "conda-env-xai-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
